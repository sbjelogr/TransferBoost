{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "single-healthcare",
   "metadata": {},
   "source": [
    "# Getting started with transferboost\n",
    "\n",
    "This is quick start tutorial providing code snippets for getting started with tboost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-reaction",
   "metadata": {},
   "source": [
    "## XGBTransferLearner: transfer learning with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "royal-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import transferboost as tb\n",
    "from transferboost.dataset import load_data\n",
    "\n",
    "# Load the data\n",
    "X, y1, y2 = load_data(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-growing",
   "metadata": {},
   "source": [
    "### Train an xgboost model and perform the \"transfer learning\"\n",
    "\n",
    "Train an `xgboost` model on the first target, with `y1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fewer-carter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=0, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    max_depth = 2,\n",
    "    reg_lambda = 0,\n",
    "    n_estimators=100,\n",
    "    verbosity = 0\n",
    ")\n",
    "\n",
    "xgb_model.fit(X,y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-fishing",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Transfer Learners expect a fitted model in the constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "champion-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transferboost.models import XGBTransferLearner\n",
    "\n",
    "t_xgb_model = XGBTransferLearner(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-interference",
   "metadata": {},
   "source": [
    "Perfrom the \"transfer learning\" by fitting the XGBTransferLearner on another target, `y2` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "close-retrieval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBTransferLearner with base model\n",
       "\tXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=0, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=0)\n",
       "base_score = 0.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_xgb_model.fit(X,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-spirit",
   "metadata": {},
   "source": [
    "### Get the predicted probabilities with the transfer-learned model.\n",
    "\n",
    "`XGBTransferLearner.predicted_proba(X)` returns the the probabilities (as in any sklearn API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "assumed-choice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70023316, 0.29976684],\n",
       "       [0.70499377, 0.29500623],\n",
       "       [0.91163901, 0.08836099],\n",
       "       ...,\n",
       "       [0.60772859, 0.39227141],\n",
       "       [0.76717276, 0.23282724],\n",
       "       [0.80914091, 0.19085909]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_xgb_model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-greeting",
   "metadata": {},
   "source": [
    "## LGBMTransferLearner: transfer learning with ligthgbm\n",
    "\n",
    "If the baseline model is a lightgbm, the transfer learning procedure is very similar.\n",
    "\n",
    "\n",
    "### Train a lightgbm model and perform the \"transfer learning\"\n",
    "As in the xgboost case, train a LGBM classifier on target `y1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "universal-magnet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(max_depth=2, reg_lambda=0, verbosity=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    max_depth = 2,\n",
    "    reg_lambda = 0,\n",
    "    n_estimators=100,\n",
    "    verbosity = 0\n",
    ")\n",
    "\n",
    "lgb_model.fit(X,y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-matter",
   "metadata": {},
   "source": [
    "Use the `LGBMTransferLearner` class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inside-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transferboost.models import LGBMTransferLearner\n",
    "\n",
    "t_lgb_model = LGBMTransferLearner(lgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-winning",
   "metadata": {},
   "source": [
    "Transfer-learn the model to the new target (`y2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hawaiian-postage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBTransferLearner with base model\n",
       "\tLGBMClassifier(max_depth=2, reg_lambda=0, verbosity=0)\n",
       "base_score = 0.3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_lgb_model.fit(X,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-probe",
   "metadata": {},
   "source": [
    "### Get the predicted probabilities with the transfer-learned model.\n",
    "\n",
    "`LGBMTransferLearner.predicted_proba(X)` returns the the probabilities (as in any sklearn API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "elegant-morning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67982818, 0.32017182],\n",
       "       [0.92096689, 0.07903311],\n",
       "       [0.81908308, 0.18091692],\n",
       "       ...,\n",
       "       [0.53578214, 0.46421786],\n",
       "       [0.80567013, 0.19432987],\n",
       "       [0.67593326, 0.32406674]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_lgb_model.predict_proba(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
